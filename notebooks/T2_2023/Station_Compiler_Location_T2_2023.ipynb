{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to extract data for latitude and longitude"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOLLOW THE STEPS BELOW:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below, it will prompt you to enter the file location. The file location can be a raw.githubusercontent.com or right click on the dataset and copy the file path, copy path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1 the url is given a value and is not \"None\" therefore the defined function \"load_data\" will not prompt the user to enter location.\n",
    "#url = \"https://raw.githubusercontent.com/Chameleon-company/EVCFLO/main/datasets/T1_2023/New_Zealand/NZ_Public_EV_Charger_Data_2023-04-28%2000_05_06NZDT.csv\"\n",
    "\n",
    "url=\"\"\n",
    "\n",
    "#Option 2 the above url is \"None\" unlike the example in Option 1. The following code is a defined function, when run it will prompt you to enter a file location.\n",
    "\n",
    "def load_data(url=None):\n",
    "    if url is None:\n",
    "        url = input(\"Please enter the URL to the CSV file: \")\n",
    "    \n",
    "    data = pd.read_csv(url)\n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below, It will tell you the number of rows and columns as well as the column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 278 rows and 13 columns.\n",
      "Columns & data types in the dataset:\n",
      "Region              object\n",
      "Locality            object\n",
      "Operator            object\n",
      "Owner               object\n",
      "DC or AC            object\n",
      "kW                   int64\n",
      "Connectors          object\n",
      "Num_installed        int64\n",
      "Num_in_progress      int64\n",
      "Address             object\n",
      "Long               float64\n",
      "Lat                float64\n",
      "EECA funded         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Return the number of rows and the number fo columns from the data entered in the cell above.\n",
    "\n",
    "print(\"This dataset has\", data.shape[0], \"rows and\", data.shape[1], \"columns.\")\n",
    "\n",
    "#Return the data types of each variable (column) from the data provided.\n",
    "\n",
    "print(\"Columns & data types in the dataset:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below, it will prompt you to enter the column for station name / location, latitude, and longitude. use the list above to pick the correct columns or open the CSV file and find the names for the right columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 278 rows and 3 columns.\n",
      "Columns & data types in the dataset:\n",
      "Service_Station_Location     object\n",
      "Latitude                    float64\n",
      "Longitude                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Name of variable (column) representing station location\n",
    "station = input(\"Enter the column name for Station_Location: \")\n",
    "\n",
    "#Name of variable (column) representing Latitude\n",
    "latitude = input(\"Enter the column name for Latitude: \")\n",
    "\n",
    "#Name of variable (column) representing longitude\n",
    "longitude = input(\"Enter the column name for Longitude: \")\n",
    "\n",
    "#Create a new dataframe with the columns defined above\n",
    "new_df = pd.DataFrame(data, columns=[station, latitude, longitude])\n",
    "\n",
    "#Rename this new columns to the following\n",
    "new_df = new_df.rename(columns={\n",
    "    station: \"Service_Station_Location\",\n",
    "    latitude: \"Latitude\",\n",
    "    longitude: \"Longitude\"\n",
    "})\n",
    "\n",
    "#Return the number of rows and the number fo columns from the data entered in the cell above.\n",
    "print(\"This dataset has\", new_df.shape[0], \"rows and\", new_df.shape[1], \"columns.\")\n",
    "\n",
    "\n",
    "#Return the data types of each variable (column) from the data provided.\n",
    "print(\"Columns & data types in the dataset:\")\n",
    "print(new_df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below can be used if you would like to create a new csv file to store the new data to, otherwise the code cell after will ask for the file you would like to add to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.to_csv(\"Paste location including a file name for your new csv, example: file location/blank.csv\", index=False)\n",
    "\n",
    "#save_location = input(\"Enter the save location for the blank CSV file: \")\n",
    "\n",
    "#new_df.to_csv(save_location, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below, It will prompt you to enter the file location of the CSV file that you would like to add the data to. This is the same process as the first code cell except we are picking the csv file that we want to add on to. It will add the data we collected using the pandas concat function, don't worry about accidently adding duplicates it will handle these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows before dropping:\n",
      "                                Service_Station_Location   Latitude  \\\n",
      "175                     1 Stevens Grove, Lower Hutt 5010 -41.211143   \n",
      "176                        100 Pah Rd, Auckland 1023, NZ -36.912204   \n",
      "177                  103 Bridge Street, Karamea 7893, NZ -41.260034   \n",
      "178          114-124 Jackson St, Petone, Lower Hutt 5012 -41.224706   \n",
      "179                    120 Hobsonville Rd, Auckland 0618 -36.799100   \n",
      "...                                                  ...        ...   \n",
      "88635                             Roberts St, Taupo 3300 -38.690770   \n",
      "88636     10 Wharf Street, Dunedin Central, Dunedin 9016 -45.880000   \n",
      "88637           120 Kaiwaka Mangawhai Road, Kaiwaka 0975 -36.160000   \n",
      "88638                    298 State Highway 1, Bulls 4894 -40.170000   \n",
      "88639  Cheapa Campa, 575 Memorial Avenue, Burnside, C... -43.480000   \n",
      "\n",
      "        Longitude  \n",
      "175    174.904344  \n",
      "176    174.770478  \n",
      "177    172.132062  \n",
      "178    174.871332  \n",
      "179    174.644540  \n",
      "...           ...  \n",
      "88635  176.074589  \n",
      "88636  170.500000  \n",
      "88637  174.440000  \n",
      "88638  175.380000  \n",
      "88639  172.550000  \n",
      "\n",
      "[550 rows x 3 columns]\n",
      "Duplicated rows after dropping:\n",
      "Empty DataFrame\n",
      "Columns: [Service_Station_Location, Latitude, Longitude]\n",
      "Index: []\n",
      "Number of duplicates before dropping: 550\n"
     ]
    }
   ],
   "source": [
    "# Enter the file location for the CSV file to ADD the new data to\n",
    "from_df_location = input(\"Enter the file location for the CSV file: \")\n",
    "\n",
    "# Load CSV to ADD data to\n",
    "from_df = pd.read_csv(from_df_location)\n",
    "\n",
    "# Concatenate new data to chosen CSV file\n",
    "combined_df = pd.concat([from_df, new_df], ignore_index=True)\n",
    "\n",
    "# Count the number of duplicate rows based on specified columns\n",
    "num_duplicates_before = combined_df.duplicated(subset=['Service_Station_Location', 'Latitude', 'Longitude'], keep=False).sum()\n",
    "\n",
    "# Find and print duplicated rows before dropping\n",
    "duplicated_rows_before = combined_df[combined_df.duplicated(subset=['Service_Station_Location', 'Latitude', 'Longitude'], keep=False)]\n",
    "print(\"Duplicated rows before dropping:\")\n",
    "print(duplicated_rows_before)\n",
    "\n",
    "# Drop duplicates\n",
    "combined_df.drop_duplicates(subset=['Service_Station_Location', 'Latitude', 'Longitude'], inplace=True)\n",
    "\n",
    "# Find and print duplicated rows after dropping\n",
    "duplicated_rows_after = combined_df[combined_df.duplicated(subset=['Service_Station_Location', 'Latitude', 'Longitude'], keep=False)]\n",
    "print(\"Duplicated rows after dropping:\")\n",
    "print(duplicated_rows_after)\n",
    "\n",
    "# Warning! Overwrite the existing dataset with the combined dataset\n",
    "combined_df.to_csv(from_df_location, index=False)\n",
    "\n",
    "print(f\"Number of duplicates before dropping: {num_duplicates_before}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below, We can now check to see the shape of the dataset that we have added more data to. Here we can track the number of station locations indicated by the number of rows! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 88362 rows and 3 columns.\n",
      "Columns & data types in the dataset:\n",
      "Service_Station_Location     object\n",
      "Latitude                    float64\n",
      "Longitude                   float64\n",
      "dtype: object\n",
      "                            Service_Station_Location   Latitude   Longitude\n",
      "0  9 Murray Rose,SYDNEY OLYMPIC PARK NSW 2127,AUS... -33.845898  151.069768\n",
      "1           76 Cowper St,WALLSEND NSW 2287,AUSTRALIA -32.902780  151.669841\n",
      "2  Hunter Valley Gardens 2090 Broke Road,POKOLBIN... -32.773929  151.293163\n",
      "3  Cnr Hume Highway & Bessemer Street,MITTAGONG N... -34.449540  150.442926\n",
      "4              140 Queen St,BERRY NSW 2535,AUSTRALIA -34.775939  150.700542\n"
     ]
    }
   ],
   "source": [
    "#This is now the CSV file with new data added to it\n",
    "print(\"This dataset has\", combined_df.shape[0], \"rows and\", combined_df.shape[1], \"columns.\")\n",
    "\n",
    "print(\"Columns & data types in the dataset:\")\n",
    "print(combined_df.dtypes)\n",
    "print(combined_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
